\documentclass[twoside]{article}

\usepackage{amsmath,amsthm,amssymb,epsfig}
\usepackage{hyperref}
\usepackage[numbers]{natbib}
\usepackage{textcomp}
\usepackage{listings}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newenvironment{pf}{{\noindent\sc Proof. }}{\qed}
\newenvironment{map}{\[\begin{array}{cccc}} {\end{array}\]}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\unif}{\operatorname{unif}}

\newcommand{\comment}[1]{}
\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem*{exmp}{Example}
\newtheorem*{prob}{Problem}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}
\newtheorem*{exer}{Exercise}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\everymath{\displaystyle}

\newcommand{\widgraph}[2]{\includegraphics[keepaspectratio,width=#1]{#2}}
\newcommand{\widgraphr}[3]{\includegraphics[keepaspectratio,width=#1,angle=#3]{#2}}

\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf ML Reading Group (Fall 2015) \hfill Lecture: #3} }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1  \hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\it Lecturer: #2 \hfill Scribe: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}


%%%%%%%
% Some commonly used notation
%%%%%%%

\def\R{{\mathbb R}}
\def\X{{\mathcal X}}
\def\Y{{\mathcal Y}}
\def\E{{\mathbb E}}
\def\pr{{\mathbb P}}
\def\sign{{\rm sign}}

\newcommand{\percent}{$\%$}

\begin{document}

\lstset{
  basicstyle=\ttfamily,
  mathescape
}

\lecture{Learning Feature Representations with K-means}{Krishnan Srinivasan}{5}{Jonathon Cai}

\section{Today}

-- Paper Discussion

\section{Derivation of $\argmax$ term}

We would like to compute 

\[
\min_{s^{(i)}} ||Ds^{(i)} - x^{(i)}||^2_2
\]

given $D \in \R^{n \times k}$, $x^{(i)} \in \R^n$, and $s^{(i)} \in \R^k$.
The $j$-th column vector of $D$ is denoted by $D^{(j)}$, where $\forall j$, the $l_2$ norms of $D^{(j)}$ are 1 and $s^{(i)}$ has at most one nonzero coordinate. It turns out that both these 
assumptions -- the restriction on $D^{(j)}$ and the fact that $||s^{(i)}||_0 \leq 1$ -- are important for the analysis.

For the moment assume that $s^{(i)}$ has a nonzero coordinate at $r = s^{(i)}_j$, where $r$ is a constant number.
Then $Ds^{(i)} = rD^{(j)}$. So we would like to solve the equation:

\[
\dfrac{\partial}{\partial r} ||rD^{(j)} - x^{(i)}||^2_2 = 0
\]

Expanding yields 

\begin{align*}
\dfrac{\partial}{\partial r} \sum_k (rd_k - x_k)^2 &= 0\\
\sum_k 2(rd_k - x_k)d_k &= 0\\
r \sum_k  d^2_k &= \sum_k x_k d_k \\
r &= \sum_k x_k d_k && \textrm{normalized } D^{(j)} \textrm{ assumption}\\
r &= {D^{(j)}}^T x^{(i)}
\end{align*}

Now we will consider the minimum of the norm over all $j$. Let $r_j$ denote
the $r$ corresponding to $j$. Note that:

\begin{align*}
\sum_k {(r_j d_k - x_k)}^2 &= \sum_k r^2_j d^2_k + x^2_k - 2r_j d_k x_k\\
 &= r^2_j + \sum_k x^2_k - 2r^2_j\\
 &= C - r^2_j && C = l_2 \textrm{ norm squared of } x^{(i)}\\
\end{align*}

Thus, for $||s^{(i)}||_0 \leq 1$, the norm is minimized when we maximize $|r_j|$, or
when $j == \argmax_j |r_j|$.

Consider the geometric meaning of this statement. ${D^{(j)}}^T \cdot x^{(i)} = |D^{(j)}||x^{(i)}| \cos \theta = K \cos \theta$, where $K$ is a constant. Hence, 
by maximizing $\cos \theta$, we are trying to minimize $\theta$, or the angle between the projection of $x^{(i)}$ onto the unit sphere and a centroid lying
on the sphere.

\section{ZCA Whitening}

Suppose $X$ is a random vector with mean 0 and the covariance matrix $\E[X X^T]$ is positive definite (and hence invertible); since
$V$ is also symmetric, we may decompose it, using the spectral decomposition, into $V = LDL^T$, where $L$ is orthogonal and $D$ 
represents a diagonal matrix containing the eigenvalues. Let $K = \sqrt{{(D + \epsilon)}^{-1}} \approx \sqrt{D^{-1}}$.
Recall that $K^T = D$ as $D$ is diagonal. Also $K^2 D = I$ by definition. 

Now consider the ZCA transformation denoted by the transformation $T(X) = LKL^{-1}X$. Let the transformed variable be $\tilde{X}$ and consider the variance of this term. 
$\E$ is of course in front of each term.

\begin{align*}
\tilde{X}\tilde{X}^T &= LKL^{-1}LDL^TL^{-T}KL^T\\
 &= LL^T\\
 &= I
\end{align*}

Thus, ZCA whitening approximately results in spherical variance.

\end{document}
